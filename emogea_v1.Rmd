---
title: "EMOGEA: Introduction and Setup"
subtitle: "EMOGEA (Error Modelled Gene Expression Analysis)"
abstract: Error Modelled Gene Expression Analysis (EMOGEA), a framework for
          analyzing RNA-seq data that incorporates measurement uncertainty, while 
          introducing a special formulation for modeling data that are acquired as 
          a function of continuous variables. This introductory vignette provides
          an overview of the steps of the workflow, installation, and source of
          the dataset used as example.
output:
  BiocStyle::html_document:
    number_sections: no
    toc: yes
    css: corrected.css
  html_notebook:
    toc: yes
  pdf_document:
    toc: yes

vignette: >
  %\VignetteIndexEntry{SCENIC Setup}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  comment = "#>"
)
```
## Introduction to EMOGEA
Error Modeled Gene Expression Analysis (EMOGEA), an R package for the analysis of RNA-Seq gene expression data. EMOGEA incorporates measurement uncertainties in the analysis of differential expression and is specifically suited for transcriptomics studies in which low-count transcripts with small fold-changes lead to significant biological effects. Such transcripts include signaling mRNAs and non-coding RNAs (ncRNA) which are known to exhibit low levels of expression. The package handles missing  values by associating disproportionately large uncertainties to those measurements, making it particularly useful for single-cell RNA-Seq measurements. It is specifically suited for ordinal data as it implements a constrained alternating least-squares (ALS) approach that allows waves of expression profiles to be visualized against the ordinal variable. For differential expression analysis, EMOGEA has a much higher true positivity rate (TPR) and a vanishingly small false negativity rate (FNR) compared to common approaches.  

## More info & citation

The approach behind EMOGEA and its application to several datasets (e.g. usage examples) will soon be available upon completion of ongoing academic peer review 

When this is available please, cite the article if you use EMOGEA in your research.

# Requirements

## Species

The current version of EMOGEA has been tested using  *human*, *mouse* and *zebrafish* data. There is no limitation to the model organism

#-------------------------
# Setting up the data
# ------------------------

## Input: expression matrix and metadata
Two inputs are required for EMOGEA: (a) a bulk or single-cell RNA-seq **expression matrix** and a **metadata file** such that:

- Each column corresponds to a sample (cell) and each row corresponds to a gene.

- The gene ID can be either **gene-symbol** or **ENSEMBLID** and stored as `rownames`.

- Rownames for the metadata should exactly match column names for the **expression matrix** and indicate which columns are replicates.

- For **Temporal** (or ordinal) measurements, the time/variable must be provided in the **metadata file**

- Expression **units**: The preferred expression values are gene-summarized counts. 
We strongly recommendation using the *raw* counts, or for scRNAseq, counts *normalized* through single-cell specific methods (e.g. Seurat). 
Other measurements, such as transcripts/counts per million (TPM) and FPKM/RPKM, are also accepted as input.
The choice of input expression matrix might have some effect on the co-expression analysis to create the regulons (step 1). The other steps of the workflow are not directly affected by the input expression values: (2) The expression is not taken into account for the motif analysis, and (3) AUCell, which is used for scoring the regulons on the cells, is cell ranking-based (it works as an implicit normalization).

We start with a count matrix where each row is a gene and each column is a sample (for bulk RNAseq) or a single cell (for scRNAseq). 
These can be obtained by mapping read sequences to a reference genome and then counting the number of reads mapped to the exons of each gene. 
(See, for example, the Rsubread package which does both tasks.) Alternatively, pseudo-alignment methods can be used to quantify the abundance of each transcript in each sample.  Alternatively, we can also accept data arising from Affymetrix arrays. In this example we will use yeast data generated by  Gierliński et al., [PMID: 26206307].

# Installation

To install the package, type the following in the R console: 
```R
devtools::install_github("itikadi/emogeav1", build_vignettes = TRUE)
```
# Analysis of Binary (Case-Control) Data
#
In this example we will use a highly replicated yeast data generated by  Gierliński et al., [PMID: 26206307].
 
```{r echo = T, eval = F}


library(EMOGEA)
data(yeastExample)
#data(bottomlyExample)
#---------- (i) Get variables ------------------
#
expressionData <- yeastExample$expressionData 
metaData <- yeastExample$metaData
sampleColumn <- yeastExample$sampleColumn
conditionColumn <- yeastExample$conditionColumn

```
## Prepare data 
This step prepares input data for the EMOGEA curve resolution function that calculates the profiles of a
gene expression matrix measured as a function of time. The input data is an expression matrix which must
have metadata with replicate information. If there are no replicates, the total least squares implementation
of the algorithm cannot be performed and instead, an unweighted ordinary alternating least squares  implementation is used.
The output is a list, prepareDataOutput, that contains: the processed expression data (expressionMatrix),
the residual matrix (residualMatrix), and the error covariance matrix (errorCovarianceMatrix). More details of the pre-processing
performed by this function can be found in the help file for the function itself.

 
```{r echo = T, eval = F}

prepareDataOutput <- prepareData(
  expressionData = expressionData,
  metaData = metaData,
  sampleColumn = sampleColumn,
  conditionColumn = conditionColumn,
  applyLogTransformation = FALSE)
#
```

## Maximum Likelihood Projection

At its most basic form, Maximul Likelihood PCA can be viewed as a superset of the classical PCA that is weighted by measurement 
 errors that de-emphasize noisy measurements. It is however more sophisticated since it incorporates measurement errors of different
 structures ranging from the basic homoscedastic (iid normal) to more complex heteroscedastic noise with different correlation structures. mlProjection performs this MLPCA and requires the following variables.
 
### ML Projection input 
 
 - *expressionMatrix*" the expression matrix organized as (n X m), where n is the number of genes and m is the number of samples. This input arises from the output of prepareData() as  decribed in 1(ii) above.
 
- *errorCovarianceMatrix*: The inverse of the measurement errors, whose dimensions are (*n* X *n*). You should use the errorCovarianceMatrix output of prepareData().

- *numberOfComponents*: the number of principal components estimated to be required to reconstruct the expression matrix without loss of generality (default =15).

- *maxiter*: The maximum number of iterations to peform during the computation of the Maximum Likelihood. Once the max iteration is reached,the algorithms stops (default=2000).

- *tolerance*: The termination tolerance for the computation of the Maximum Likelihood. Once the improvements made by Maximum Likelihood reach the tolerance, the algorithm stops (default =1e-10).
- verbose Whether to display the information about the computation or not.(default=TRUE)

### ML Projection output 

A list consisting of **U** *(principal components)*, **S** *(eigenvalues)* and **V** *(loadings)*.
The estimated matrix (estimatedMatrix) is given as t(U %*% S %*% t(V)).

```{r echo = T, eval = F}
projectionOutput <- mlProjection(
  expressionMatrix = prepareDataOutput$expressionMatrix,
  errorCovarianceMatrix = prepareDataOutput$errorCovarianceMatrix)
# Check results
projectionOutput$U[1:5, 1:5]
projectionOutput$S[1:5, 1:5]
projectionOutput$V[1:5, 1:5]
projectionOutput$estimatedMatrix[1:5, 1:5]
```
#
##  Visualizing the Output
#
```{r echo = T, eval = F}

ncomp<-15
u<-projectionOutput$U[,1:ncomp]
s<-projectionOutput$S[1:ncomp,1:ncomp]
MLproj<-as.data.frame(u%*%s)
pca <- prcomp(t(projectionOutput$estimatedMatrix))
dmlpca <- data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2], PC3 = pca$x[,3], group = coldata$batch, 
                     name = coldata$strain)
dmlpca <- data.frame(PC1 = MLproj$Comp1, PC2 = MLproj$Comp2, PC3 = MLproj$Comp3, group = coldata$batch, 
                     name = coldata$strain)
#
mlpcaPlot<-ggplot(dmlpca, aes(PC2, PC3, color=name, shape=group)) +   geom_point(size=4) +
  xlab(paste0("PC2: ",percent[1],"% variance")) +
  ylab(paste0("PC3: ",percent[1],"% variance"))
mlpcaPlot
```
#
# Analysis of Temporal Data 
#
## Error Weighted Self-modelling Curve Resolution 
#
Two parameters are important here and must be specified a prior. First is the number of components to be extracted. For this example we use 5 but this varies and can be determined computationally by analyzing the residual sum of squares done sequentially until an appropriate number is determined.  In principle, one can add as many components as there are samples, but degenerate solutions begin to emerge when the true number of unique profiles has been reached and, if an absurd number of profiles is specified, the resulting ill-conditioned matrix will be accompanied by non-convergence of the algorithm to which you will be alerted about.
The second parameter is the tolerance value which by default is set at 0.00009 and is used to determine convergence. Here we use zebrafish data from White et al. [PMID: 29144233]

```{r echo = T, eval = F}
data(temporalData)
library(Matrix)
ncomp=5 
curveResOutput <- multivariateCurveResolution(expressionMatrix = prepareDataOutput$expressionMatrix,
                                              residualMatrix = prepareDataOutput$residualMatrix,
                                              numberOfComponents = ncomp,tolerance = 0.00009)
# Check output
curveResOutput$Popt[1:3, 1:ncomp]
curveResOutput$Copt[1:ncomp, 1:3]
curveResOutput$estimatedMatrix[1:ncomp, 1:ncomp]
curveResOutput$geneProfiles[1:ncomp, 1:3]
```
#
## Curve resolution without error weighting
#
If you don't have sufficient replication, you can also run curve resolution without error weighting.

```{r echo = T, eval = F}
library(Matrix)
ncomp=5 
curveResOutput_noResidualMatrix <- multivariateCurveResolution(expressionMatrix = prepareDataOutput$expressionMatrix,
                                                               numberOfComponents = ncomp)
# Check output
curveResOutput_noResidualMatrix$Popt[1:3, 1:ncomp]
curveResOutput_noResidualMatrix$Copt[1:ncomp, 1:3]
curveResOutput_noResidualMatrix$estimatedMatrix[1:ncomp, 1:ncomp]
curveResOutput_noResidualMatrix$geneProfiles[1:ncomp, 1:3]
```
#
## Visualizing the Output of Curve Resolution
#
The outpy of **multivariateCurveResolution** has optimized matrices **C** and **P** that, respectively, contain temporal profiles and genes associated with each profile. For visualizing the profiles, plot the columns of **C** against time or whatever ordinal variable was tested in the experiment

```{r echo = T, eval = F}
#
#
C<-curveResOutput$Copt
C<-as.data.frame(C)
tim<-as.numeric(gsub("ZFS.","",metaData$condition))
#
library(purrr)
library(ggplot2)
library(cowplot)
#
norm.scale <- function(x){x/sqrt(sum(x^2))}     # normalization to unit length
#
x_ord<-c(4,3,2,5,1,6)                           # order the profiles This is a personal preference, you can order them any way you like
#
C<-C[,x_ord]
C<-purrr::set_names(C, paste0("Prof_",1:ncol(C)))
for (i in 1:dim(C)[2]){
  C[,i]<-norm.scale(C[,i])
}
xdata<-as.data.frame(cbind(tim,C))
#
myplots <- vector('list', ncol(C))
plot_data_column = function (data, column) {
  ggplot(data, aes_string(x = data[,1],y=column)) +
    geom_smooth(method = lm, formula = y ~ splines::bs(x, 6), color="red", fill="#69b3a2",se=TRUE)+
    theme_classic()+
    xlab("Time (hrpf)" )+
    ylab("Expression Profiles")
}
#
myplots <- lapply(colnames(C), plot_data_column, data = xdata)
plot_grid(plotlist=myplots, labels = "AUTO")

```
##  Correlation with Gene Profiles
As a bonus, we also include gene annnotation to allow for downstream functional analysis

```{r echo = T, eval = F}
# Start with gene annotation
#----------------------------------------Gene annotations using biomaRt------
#
library(biomaRt)
species<-"zebrafish" # you must change this if you're not analysing zebrafish data
#
if(tolower(species)=="zebrafish"){spec="rerio"}
ensembl<-biomaRt::useEnsembl(biomart = "ensembl", mirror = "uswest")
ensembl <-biomaRt::useDataset(biomaRt::searchDatasets(mart = ensembl, pattern = spec)$dataset[1],mart=ensembl)
#
filterValues <- as.character(expressionData$V1)
filterType<-{}
featureIdformat<-"ensemblid"
if(tolower(featureIdformat)=="symbol"){filterType="external_gene_name"}
if(tolower(featureIdformat)=="entrezid"){filterType="entrezgene_id"}
if(tolower(featureIdformat)=="ensemblid"){filterType="ensembl_gene_id"}
#
attributeNames <- c('ensembl_gene_id','entrezgene_id','external_gene_name','chromosome_name','start_position','end_position','strand')
annot <- biomaRt::getBM(attributes=attributeNames,
                        filters = filterType,
                        values = filterValues,
                        mart = ensembl)
colnames(annot)<-c("ENSEMBLID","ENTREZID","SYMBOL","Chr","start","end","strand")
xdata<-cbind(rownames(xdata),xdata)
colnames(xdata)[1]<-toupper(featureIdformat)
#
non_na_idx <- which(is.na(annot$SYMBOL) == FALSE)
# Return only the genes with annotations using indices
annot <- annot[non_na_idx, ]
xdata2<-merge(annot,xdata)

#
#-----------------------Calculate the cosine similarity between gene expression profiles and MCR output----
#
Copt<-curveResOutput$Copt[,x_ord]
#
Xorg<-t(as.matrix(xdata2[,8:dim(xdata2)[2]]))
Genes<-xdata2[,1:7]
ngenes <- ncol(Xorg)
angles <- matrix(0,ncomp,ngenes)
similarity<-angles
#
for (j in 1: ncomp)
{
  c<-norm.scale(Copt[,j])
  j
  for (i in 1:ngenes)
  {
    s<-norm.scale(Xorg[,i])
    similarity[j,i] = cosine(Xorg[,i], Copt[,j]) # Cosine Similarity
    angles[j,i]=cor(c,s) # Correlation coefficient
    }
}
#
angles<-as.data.frame(similarity) # We use cosine similarity instead of correlation coefficient
colnames(angles)<-xdata2$ENSEMBLID
delete.na <- function(DF, n=0) {
  DF[,colSums(is.na(DF)) <= n]
}
newAngles<-delete.na(angles)
#
geneProfiles<-data.frame((matrix(vector(),dim(newAngles)[2],ncomp,
                                 dimnames = list(c(),colnames(C)))))
sortedAngles<-geneProfiles
for (i in 1:ncomp){
  k<-sort(newAngles[i,],decreasing = T,na.last=TRUE)
  geneProfiles[,i]<-names(k)
  sortedAngles[,i]<-t(k)
  rm(k)
}
##
Gene.annotations<-Genes
prof.name<-purrr::set_names(geneProfiles, paste0("Prof_",1:ncol(geneProfiles)))
#
#----------Export gene profiles as an option----------
#
for (j in 1:ncomp)
{
  xb<-as.data.frame(cbind(geneProfiles[,j],sortedAngles[,j]))
  colnames(xb)<-c("ENSEMBLID","CorCoef")
  profile.annot<-plyr::join(xb,Gene.annotations)
  filNam<-paste0(dataDir,"MCR_wALSgeneProfile_",j,"_.csv")
  data.table::fwrite(profile.annot,file = filNam, sep=",")
}
#
```
# Analysis of scRNAseq data 
# 
scRNA-seq data implicitly exhibit an ordinal structure because (unless under exceptional circumstances) cells are not typically arrested at any stage of development, cell cycle or other transitions prior to sampling. Gene expression subsequently reflects this ordinal, cellular characteristic and there are several methods for ordering cells in a “pseudo time” to reflect this. For these data we assume that you have done all the preprocessing following acceptable protocols. In this example we use data generated by Deng et al., [PMID: 24408435].

```{r echo = T, eval = F}
library(scater)
# start by loading the data and re-factor the cell types
data(dengSCE)
# Re-order the levels of the factor storing the cell developmental stage.
deng_SCE$cell_type2 <- factor(deng_SCE$cell_type2,
                              levels = c("zy", "early2cell", "mid2cell", "late2cell", 
                                         "4cell", "8cell", "16cell", "earlyblast", "midblast",
                                         "lateblast"))

# Run PCA on Deng data. Use the runPCA function from the SingleCellExperiment package.
deng_SCE <- runPCA(deng_SCE, ncomponents = 50)
# Add PCA data to the deng_SCE object.
# Use the reducedDim function to access the PCA and store the results. 
pca <- reducedDim(deng_SCE, "PCA")

deng_SCE$PC1 <- pca[, 1]
deng_SCE$PC2 <- pca[, 2]
#
# Plot PC biplot with cells colored by cell_type2. 
ggplot(as.data.frame(colData(deng_SCE)), aes(x = PC1, y = PC2, color = cell_type2)) + geom_quasirandom(groupOnX = FALSE) +
  scale_color_tableau() + theme_classic() +
  xlab("PC1") + ylab("PC2") + ggtitle("PC biplot")
#
#
deng_SCE$pseudotime_PC1 <- rank(deng_SCE$PC1)  # rank cells by their PC1 score
ggplot(as.data.frame(colData(deng_SCE)), aes(x = pseudotime_PC1, y = cell_type2, 
                                             colour = cell_type2)) +
  geom_quasirandom(groupOnX = FALSE) +
  scale_color_tableau() + theme_classic() +
  xlab("PC1") + ylab("Timepoint") +
  ggtitle("Cells ordered by first principal component")
```
#

## Estimate the Pseudotime with principal curves
#
Start by ordering the cells in pseudo time by performing PCA and plotting the data projections in first two principal components (PCs). Following this, determine a principal curve through the data cloud and obtain the orthogonal projections of the data onto this principal curve and order the cells according to the principal curve projections. Estimation of the principal curves is iterative but you can turn that off if you wish.

```{r echo = T, eval = F}
graphics.off()
plot(pca[,1],pca[,2])
x1 <- pca[,1]
x2<-pca[,2]
x <- cbind(x1,x2)
alim <- extendrange(x, f=0.1)
alim_ <- range(x)
## plot centered data
dev.off()
plot(x[,1], x[,2], bty='n',
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     xlim=alim, ylim=alim)
legend("topleft", legend=c("Initialize"), bty="n")
## plot first principal component line
svdx <- svd(x)
clip(alim_[1],alim_[2],alim_[1],alim_[2])
with(svdx, abline(a=0, b=v[2,1]/v[1,1]))
## plot projections of each point onto line
z1 <- with(svdx, x%*%v[,1]%*%t(v[,1]))
segments(x0=x[,1],y0=x[,2],
         x1=z1[,1],y1=z1[,2])
## compute initial lambda (arc-lengths associated with
## orthogonal projections of data onto curve)
lam <- with(svdx, as.numeric(u[,1]*d[1]))
dev.off()
for(itr in 1:10) {

  #### step (a) of iterative algorithm ####

  ## compute scatterplot smoother in either dimension
  ## increase 'df' to make the curve more flexible
  #fit1 <- smooth.spline(x=lam, y=x[,1], df=4)
  #fit2 <- smooth.spline(x=lam, y=x[,2], df=4)
  fit1<-loess(x[,1]~lam,span = 0.85) # loess fit
  fit2<-loess(x[,2]~lam,span = 0.85) # loess fit
  ## plot data and the principal curve for a sequence of lambdas
  plot(x[,1], x[,2], bty='n',
       xlab=expression(x[1]),
       ylab=expression(x[2]),
       xlim=alim, ylim=alim)
  legend("topleft", legend=c("Step (a)"), bty="n")
  seq_lam <- seq(min(lam),max(lam),length.out=100)
  #lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y) # spline fitting
  lines(predict(fit1,seq_lam),predict(fit2,seq_lam)) # loess fitting
  ## show points along curve corresponding to original lambdas
  ##
  #z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
  z1<- cbind(predict(fit1, lam), predict(fit2, lam)) #loess prediction
  segments(x0=x[,1],y0=x[,2],
           x1=z1[,1],y1=z1[,2])
  #### step (b) of iterative algorithm ####
  ## recompute lambdas
  euc_dist <- function(l, x, f1, f2)
    #sum((c(predict(f1, l)$y, predict(f2, l)$y) - x)^2)
    sum((c(predict(f1, l), predict(f2, l)) - x)^2) #loess
  lam <- apply(x,1,function(x0) optimize(euc_dist,
                                         interval=extendrange(lam, f=0.40),
                                         x=x0, f1=fit1, f2=fit2)$minimum)
  ## show projections associated with recomputed lambdas
  plot(x[,1], x[,2], bty='n',
       xlab=expression(x[1]),
       ylab=expression(x[2]),
       xlim=alim, ylim=alim)
  legend("topleft", legend=c("Step (b)"), bty="n")
  seq_lam <- seq(min(lam),max(lam),length.out=100)
  #lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
  lines(predict(fit1, seq_lam), predict(fit2, seq_lam),lty = 1, lwd = 4, col = "red") #loess lines
  #z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
  z1 <- cbind(predict(fit1, lam), predict(fit2, lam)) # loess fit
  segments(x0=x[,1],y0=x[,2],
           x1=z1[,1],y1=z1[,2])

}
deng_SCE$pseudotime_PC1 <- rank(deng_SCE$PC1)  # rank cells by their PC1 score
deng_SCE$pseudotime_PrCurve<- rank(lam)
dev.off()
ggplot(as.data.frame(colData(deng_SCE)), aes(x = pseudotime_PrCurve, y = cell_type2,
                                             colour = cell_type2)) +
  #geom_point(size = 2)+
geom_quasirandom(groupOnX = FALSE) +
  scale_color_tableau() + theme_classic() +
  xlab("Principle Curve") + ylab("Timepoint") +
  ggtitle("Cells ordered by first principal curve")
#
```
#
## Curve resolution of scRNAseq data
#
In this implementation, we use pseudotime estimates from the whole data set. However, for larger data, you can do that for each cluster, i.e., determine pseudo-temporal order per cluster or a subset of clusters.

#
```{r echo = T, eval = F}
library(Matrix)
ncomp=4 # we chose 4 components here but this procedure is similar to that of bulk RNAseq

curveResOutput_noResidualMatrix <- multivariateCurveResolution(expressionMatrix = log2(as.data.frame(assay(deng_SCE))+1),
                                                               numberOfComponents = ncomp,tolerance = 0.00001)

C<-curveResOutput_noResidualMatrix$Copt
C<-as.data.frame(C)
x_ord<-c(1,4,3,2)                           # order the profiles This is a personal preference, you can order them any way you like
#
C<-C[,x_ord]
tim<-as.numeric(deng_SCE$pseudotime_PrCurve)
#
norm.scale <- function(x){x/sqrt(sum(x^2))}
C<-purrr::set_names(C, paste0("Prof_",1:ncol(C)))
for (i in 1:dim(C)[2]){
  C[,i]<-norm.scale(C[,i])
}
deng_SCE$Profile<-C
library(cowplot)
plot_data_cells = function (data, column,cols) {
  ggplot(data, aes_string(x = data[,1],y=column, colour=data[,ncomp+2])) +
    geom_point()+
    #geom_smooth(method=loess, span=0.4, color="red", fill="#69b3a2", se=TRUE) +
    geom_smooth(method = lm, formula = y ~ splines::bs(x, 6), color="red", fill="#69b3a2",se=TRUE)+
    geom_quasirandom(groupOnX = FALSE) +
    scale_color_tableau() + theme_classic() +
    xlab("PseudoTime" )+
    ylab("Cell Types")
}
celltype=deng_SCE$cell_type2
cellData<-as.data.frame(cbind(deng_SCE$pseudotime_PrCurve,deng_SCE$Profile,celltype))
colnames(cellData)[1]<-"pseudoTime"
colnames(cellData)[ncomp+2]<-"cellType"
#
myplots <- lapply(colnames(C), plot_data_cells, data = cellData)
dev.off()
plot_grid(plotlist=myplots, labels = "AUTO")
#
```
#
## Visualizing the Output of Curve Resolution
#
The outpy of **multivariateCurveResolution** has optimized matrices **C** and **P** that, respectively, contain temporal profiles and genes associated with each profile. For visualizing the profiles, plot the columns of **C** against time or whatever ordinal variable was tested in the experiment. This procedure for scRNAseq is similar to the one shown earlier.


